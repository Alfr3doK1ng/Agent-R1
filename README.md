<h1 align="center"> Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning </h1>

## Overview

**Reinforcement learning (RL)** has catalyzed the evolution of Large Language Models (LLMs) from simple **Chatbots (Level 1)** to powerful **Reasoners (Level 2)** capable of superhuman performance on complex tasks like mathematics and coding. Models trained with reinforcement learning have demonstrated remarkable abilities to develop complex reasoning strategies through exploration and exploitation, as seen in breakthrough models like DeepSeek's R1, which naturally learns to construct long reasoning chains to solve challenging problems.

The advancement of foundation models has fueled aspirations for true **Agents (Level 3)**.  Unlike chatbots and reasoners, agents not only utilize their internal knowledge but also actively explore external environments through autonomous action. Traditional agent approaches primarily rely on human-designed workflows, where models passively interact with environments according to predefined rules. While reasoners have freed us from the burden of prompt engineering through their ability to independently analyze and break down problems, a new question emerges: Can we enable models to independently take actions and explore environments on their own? The intersection of **RL & Agent** reveals this promising frontierâ€”where models learn not just to reason but to act autonomously in complex, dynamic environments.

**Agent-R1** is an open-source framework designed to accelerate research and development at this critical intersection. Our framework employs **End-to-End** reinforcement learning to train agents in specific environments. Developers need only define domain-specific tools and reward functions to extend Agent-R1 to their unique use cases, eliminating the need for complex workflow engineering. We hope our modest contribution can benefit the open-source community, making it easier for researchers and developers to create and explore agents in their own domains, collectively advancing the development of autonomous agents.

## Algorithm

Reinforcement learning for Agents differs significantly from LLM (Chatbots, Reasoners). The key distinction lies in the fact that: **a complete Agent trajectory typically involves multi-turn interaction**, requiring **multiple tool calls** to solve user queries. Below, we formalize this distinction using the Markov Decision Process (MDP) framework. 

An MDP is defined as a four-tuple $(S, A, P, R)$ where:
- $S$ is the state space
- $A$ is the action space
- $P: S \times A \times S \rightarrow [0, 1]$ is the transition probability function
- $R: S \times A \rightarrow \mathbb{R}$ is the reward function

In the context of LLM reinforcement learning:

- **State** ($s_t \in S$): The state is the sequence of the current prompt and generated text, including the input prompt and all text generated by the model so far. Formally, $s_t = [x_1, x_2, ..., x_n] + [y_1,y_2,..., y_t]$ where $x_i$ represents input tokens and $y_i$ represents generated tokens.
- **Action** ($a_t \in A$): Actions involve selecting the next token from the vocabulary $V$ to add to the current sequence, i.e., $a_t \in V$. This aligns with the autoregressive nature of LLMs, where the model predicts the next token based on the current context.
- **Transition Function** ($P$): Transitions are deterministic, as selecting a token results in a new state that is simply the current sequence plus that token. Formally, $P(s_{t+1}|s_t,a_t) = 1$ where $s_{t+1} = [s_t, a_t]$.
- **Reward Function** ($R$): Rewards are typically provided at the completion of a sequence, with a reward model evaluating the quality of the entire sequence. Formally, $R(s_t, a_t) = 0$ for $t < T$ and $R(s_T, a_T) = \mathcal{R}(s_{T+1})$ where $\mathcal{R}$ is the reward model.

For Agent reinforcement learning, the components are as follows:

- **State** ($s_t \in S$): The state at time step $t$ represents the **sequence up to the current point** in the interaction, containing the initial user query and all subsequent model outputs and tool responses generated so far. Formally, if we are currently in turn $m$ and have generated $c$ tokens in this turn:

  $s_t = [x_1, x_2, ..., x_n] + [y_1^1, y_2^1, ..., y_{t_1}^1] + [r_1^1, r_2^1, ..., r_{k_1}^1] + ... + [y_1^{m-1}, y_2^{m-1}, ..., y_{t_{m-1}}^{m-1}] + [r_1^{m-1}, r_2^{m-1}, ..., r_{k_{m-1}}^{m-1}] + [y_1^m, y_2^m, ..., y_c^m]$

  where:
  - $[x_1, x_2, ..., x_n]$ represents the initial user input tokens
  - $y_i^j$ represents tokens generated by the model in the $j$-th turn (including tool calls)
  - $r_i^j$ represents tool responses from tool calls in the $j$-th turn
  - $t_j, k_j$ represent the number of tokens in each respective sequence for completed turn $j$
  - $m$ is the current turn number (not necessarily the final turn)
  - $c$ is the number of tokens generated so far in the current turn $m$ ($c \leq t_m$)
  
- **Action** ($a_t \in A$): Actions remain the same as in conventional LLM training - selecting the next token from the vocabulary to add to the current sequence, consistent with the autoregressive nature of LLMs.
- **Transition Function** ($P$): Transitions are deterministic in most cases, where selecting a token results in a new state that is the current sequence plus that token. However, when certain special tokens **trigger tool calls**, the state undergoes a **stochastic transition**. Formally:
  - For regular tokens: $P(s_{t+1}|s_t,a_t) = 1$ where $s_{t+1} = [s_t, a_t]$
  - For tool-triggering tokens: $P(s_{t+1}|s_t,a_t) = p(r|s_t,a_t)$ where $s_{t+1} = [s_t, a_t, r]$ and $r$ is the tool response
- **Reward Function** ($R$): Rewards can be provided not only at sequence completion but also **after each tool call and feedback**. Formally:
  - For regular transitions: $R(s_t, a_t) = 0$
  - For tool call transitions: $R(s_t, a_t) = \mathcal{R}_{tool}(s_t, a_t, r)$ where $r$ is the tool response
  - For sequence completion: $R(s_T, a_T) = \mathcal{R}_{final}(s_{T+1})$


With this formal MDP representation, we can apply various reinforcement learning algorithms such as PPO, REINFORCE++, and GRPO to optimize the agent's policy $\pi_\theta(a_t|s_t)$ where $\theta$ represents the model parameters.

## Key Features

- **Multi-turn Tool Calling**: End-to-end reinforcement learning on complete interaction trajectories, allowing agents to learn from sequences of actions
- **Custom Tools and Environments**: Compatible with mainstream LLM tool calling formats, making it easy to extend with your own tools and scenarios
- **Multiple RL Algorithms**: Supports diverse reinforcement learning approaches including PPO, GRPO, and REINFORCE++
- **Reasoning before Action**: Jointly optimize reasoning and action strategies over entire trajectories.

## Upcoming Features

- **Immediate Action Rewards**: Per-action reward mechanisms to complement trajectory-level reinforcement
- **Expanded Model Support**: Integration with more foundation models beyond the currently supported Qwen
- **Additional Use Cases**: More example implementations across diverse scenarios and domains

## Get Started

### Environment Setup

**Clone the repository**
```bash
git clone https://github.com/0russwest0/Agent-R1.git
cd Agent-R1
```

**Install `verl`**
```bash
mkdir -p envs
cd envs
conda create -n verl python==3.9
conda activate verl
# install verl together with some lightweight dependencies in setup.py
pip3 install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu124
pip3 install flash-attn --no-build-isolation
git clone https://github.com/volcengine/verl.git
cd verl
pip3 install -e .
```

### Quick Start: Try Default Search Tool on HotpotQA
#### 1. Install `FlagEmbedding` and `faiss`
```bash
pip3 install FlagEmbedding
pip3 install faiss-cpu
```

#### 2. Download and preprocess HotpotQA dataset
```bash
# Create data directory
mkdir -p data/hotpotqa

# Run the preprocessing script
python examples/data_preprocess/hotpotqa.py --local_dir ./data/hotpotqa
```

This script will:
- Download the HotpotQA dataset directly from the source
- Process the data into the format required by Agent-R1
- Save the processed data as train.parquet and validation.parquet in the specified directory

#### 3. Build hotpotqa search index
```bash
# Download the corpus file (gzipped)
mkdir -p data/corpus/hotpotqa
wget https://huggingface.co/datasets/BeIR/hotpotqa/resolve/main/corpus.jsonl.gz -O data/corpus/hotpotqa/corpus.jsonl.gz

# Extract the gzipped file
gunzip -c data/corpus/hotpotqa/corpus.jsonl.gz > data/corpus/hotpotqa/hpqa_corpus.jsonl

# Process the corpus and build the search index
python scripts/hotpotqa_search/process_hotpotqa.py
```

This script will:
- Load the corpus data
- Generate embeddings using the BAAI/bge-large-en-v1.5 model
- Build a FAISS index for efficient similarity search
- Save the embeddings and index files in the data/corpus/hotpotqa directory

#### 4. Run PPO/REINFORCE++/GRPO training with Qwen2.5-1.5B-Instruct
```bash
# Run the PPO training script
bash run_ppo.sh
# Run the REINFORCE++ training script
bash run_rpp.sh
# Run the GRPO training script
bash run_grpo.sh
```

This will start the training process using the Qwen2.5-1.5B-Instruct model. The training progress can be monitored through the console output and Weights & Biases dashboard.

## Extending Agent-R1 with Your Own Tools and Environments

Agent-R1 is designed to be easily extensible, allowing you to create custom tools and environments for your specific use cases. This section outlines the key files and components you need to modify or create.

### Key Components to Extend

1. **Custom Data Processing**
   - Create a new script in `examples/data_preprocess/` following `hotpotqa.py`
   - Implement data download functions (optional, see `download_file()` in `hotpotqa.py`)
   - Create data processing functions to transform raw data into the required format:
     - Define helper functions like `